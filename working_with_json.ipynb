{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b875f1-8fd6-496b-97a5-7a1f594b1831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/21 13:45:21 WARN Utils: Your hostname, oem-Lenovo-G50-80 resolves to a loopback address: 127.0.1.1; using 192.168.0.121 instead (on interface wlp3s0)\n",
      "25/05/21 13:45:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/21 13:45:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.\\\n",
    "         builder.\\\n",
    "         appName(\"Working with json\").\\\n",
    "         master(\"local[*]\").\\\n",
    "         getOrCreate()\n",
    "        )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19eb603f-46c5-4c7b-b7fd-51285b63fbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.121:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Working with json</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a1f940a65f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70af7e2c-0e18-43ab-8f99-b84c9fcd87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read single line json\n",
    "df_single = spark.read.format(\"json\").load(\"data/input/single_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4aa28dd-5c72-4914-82fa-61bfe2d436b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a45398-60ec-41ef-935b-333d2aee2c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd82f28-9275-4dab-94b5-5a015b9a128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read multiple line json\n",
    "df_multiple = spark.read.format(\"json\").load(\"data/input/multiple_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a25a70b5-6067-4561-b0b1-2e6abf1d01d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5295/1381001330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_multiple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    }
   ],
   "source": [
    "df_multiple.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b694dd-b999-4ba2-8131-bd63a6333d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple1 = spark.read.format(\"json\").option(\"multiline\", True).load(\"data/input/multiple_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fcaffb8-c310-4cc3-9529-12a3576a1b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multiple1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16129785-9a9c-4625-8269-992595608738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"text\").load(\"data/input/single_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc016724-66ee-451f-b751-a910a3f3ae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d88741e4-a1f1-45b8-b5c6-e4bd02b9938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"order_id\":\"O101...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7956699-4e33-485b-9598-9efed7173887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"order_id\":\"O101\",\"customer_id\":\"C001\",\"order_line_items\":[{\"item_id\":\"I001\",\"qty\":6,\"amount\":102.45},{\"item_id\":\"I003\",\"qty\":2,\"amount\":2.01}],\"contact\":[9000010000,9000010001]}|\n",
      "|                                                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f579edc6-6698-4ab0-83cb-dd14a726870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = \"customer_id string, order_id string, contact array<long>\"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbd5fe96-2076-414e-aec1-7ddc0ae3018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = spark.read.format(\"json\").schema(_schema).load(\"data/input/single_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c717a42-75b9-4349-8949-f1e73a62f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+\n",
      "|customer_id|order_id|             contact|\n",
      "+-----------+--------+--------------------+\n",
      "|       C001|    O101|[9000010000, 9000...|\n",
      "+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "831723c6-1645-4d2a-a6a9-7a2d5a6cb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema_new = \"customer_id string, order_id string, contact array<string>, order_item_line array<struct<amount double,item_id string, qty long>>\"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f70351f5-ac91-44fc-8e97-9370762346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema_new = spark.read.format(\"json\").schema(_schema_new).load(\"data/input/single_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "231c21b1-5d7f-4cfd-b4c7-f9e642916386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+---------------+\n",
      "|customer_id|order_id|             contact|order_item_line|\n",
      "+-----------+--------+--------------------+---------------+\n",
      "|       C001|    O101|[9000010000, 9000...|           NULL|\n",
      "+-----------+--------+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_schema_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48c708d3-1e28-4de9-9fba-d4f2bb5ab932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fucntion from_json to read from a column\n",
    "from pyspark.sql.functions import from_json\n",
    "_schema_= \"customer_id string, order_id string, contact array<string>, order_item_line array<struct<amount double,item_id string, qty long>>\"; \n",
    "\n",
    "df_extended = df.withColumn(\"parsed\",from_json(df.value,_schema_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fa03509-478a-4948-a108-3d6f52f585b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- parsed: struct (nullable = true)\n",
      " |    |-- customer_id: string (nullable = true)\n",
      " |    |-- order_id: string (nullable = true)\n",
      " |    |-- contact: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- order_item_line: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- amount: double (nullable = true)\n",
      " |    |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_extended.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440168cd-9418-4050-8cab-0dc73dd00552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
