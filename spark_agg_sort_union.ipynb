{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c60e5a-7fd1-485b-88a1-d3e617b2296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 09:53:10 WARN Utils: Your hostname, oem-Lenovo-G50-80 resolves to a loopback address: 127.0.1.1; using 192.168.0.121 instead (on interface wlp3s0)\n",
      "25/05/20 09:53:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 09:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.\\\n",
    "         builder.\\\n",
    "         appName(\"Sorting and Agregation\").\\\n",
    "         master(\"local[*]\").\\\n",
    "         getOrCreate()\n",
    "        )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74289482-e30e-497a-b8b0-c70c07ca84f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.121:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sorting and Agregation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a1220ba2b30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d368dc-a02d-4316-b620-e9d265599a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data_1 = [\n",
    "    [\"001\", \"101\", \"John Doe\", \"30\", \"Male\", \"50000\", \"2015-01-01\"],\n",
    "    [\"002\", \"101\", \"Jane Smith\", \"25\", \"Female\", \"45000\", \"2016-02-15\"],\n",
    "    [\"003\", \"101\", \"Bob Brown\", \"35\", \"Male\", \"55000\", \"2014-01-10\"],\n",
    "    [\"004\", \"101\", \"Alice Lee\", \"29\", \"Female\", \"48000\", \"2013-04-01\"],\n",
    "    [\"005\", \"102\", \"Jack Chan\", \"40\", \"Male\", \"60000\", \"2013-04-01\"],\n",
    "    [\"006\", \"102\", \"Jill Wong\", \"32\", \"Female\", \"52000\", \"2018-07-01\"],\n",
    "    [\"007\", \"102\", \"James Johnson\", \"42\", \"Male\", \"70000\", \"2012-03-15\"],\n",
    "    [\"008\", \"102\", \"Kate Kim\", \"29\", \"Female\", \"51000\", \"2019-10-01\"],\n",
    "    [\"009\", \"103\", \"Tom Tan\", \"34\", \"Male\", \"50000\", \"2016-06-01\"],\n",
    "    [\"010\", \"103\", \"Lisa Lee\", \"27\", \"Female\", \"47000\", \"2018-08-01\"]\n",
    "]\n",
    "\n",
    "\n",
    "emp_data_2 = [\n",
    "    [\"011\", \"104\", \"David Park\", \"38\", \"Male\", \"65000\", \"2015-11-01\"],\n",
    "    [\"012\", \"105\", \"Susan Chen\", \"31\", \"Female\", \"54000\", \"2017-02-15\"],\n",
    "    [\"013\", \"106\", \"Brian Lim\", \"45\", \"Male\", \"75000\", \"2011-09-01\"],\n",
    "    [\"014\", \"106\", \"Emily Lee\", \"28\", \"Female\", \"53000\", \"2017-03-01\"],\n",
    "    [\"015\", \"106\", \"Michael Kee\", \"36\", \"Male\", \"62000\", \"2014-06-01\"],\n",
    "    [\"016\", \"107\", \"Kelly Zhang\", \"29\", \"Female\", \"49000\", \"2018-04-01\"],\n",
    "    [\"017\", \"108\", \"George Wang\", \"34\", \"Male\", \"60000\", \"2015-03-01\"],\n",
    "    [\"018\", \"108\", \"Nancy Liu\", \"30\", \"Female\", \"52000\", \"2016-01-01\"],\n",
    "    [\"019\", \"109\", \"Steven Chen\", \"37\", \"Male\", \"62000\", \"2015-08-01\"],\n",
    "    [\"020\", \"102\", \"Grace Kim\", \"32\", \"Female\", \"53000\", \"2018-11-01\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96726b4-bc7c-44af-9697-923e30a902e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a600a521-0d68-44d7-a355-5251f795af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da17390-2508-4272-b622-461c0fe741d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emp1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7593/1159878818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'emp1' is not defined"
     ]
    }
   ],
   "source": [
    "emp1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec156f-11db-4632-9751-48e97f126d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframes\n",
    "\n",
    "emp1 = spark.createDataFrame(data = emp_data_1, schema=emp_schema)\n",
    "emp2 = spark.createDataFrame(data = emp_data_2, schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4916ce8-6838-49d3-a4b7-1005ddf347a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe396ec3-2c61-4973-b95e-b3572f2000e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb35f2b-2758-4979-aea0-1244db62f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#union an dunionall   #same schema\n",
    "#union : unique or dstinct records \n",
    "#same schema but diffrent column sequence still we can do union of it\n",
    "emp_final = emp1.union(emp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0682e-a445-4a2c-8ee1-fe0876d275da",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1dcc2-92f0-46db-83e9-698a8c2df996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#union all\n",
    "#keepa duplicates \n",
    "#deprecated\n",
    "emp_final1 = emp1.unionAll(emp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0004a-96e1-4875-a2b6-d81e68ed5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_final1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f932c-c4d8-4d21-bfc0-77a3624f2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by dec saalry and order by slary\n",
    "from pyspark.sql.functions import col, asc,desc\n",
    "emp_sorted = emp_final.orderBy(col(\"salary\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c04728-ef4d-44ec-a67e-0bb20403539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5691b0-8c5b-4336-aa1d-f4e7161d6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by dec saalry and order by slary\n",
    "from pyspark.sql.functions import col, asc,desc\n",
    "emp_sorted_asc = emp_final.orderBy(col(\"salary\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc265924-02da-4754-a1ab-8ec1f0121bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_sorted_asc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a048c2-ae87-44ff-aed5-f9ce90351b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregation\n",
    "\n",
    "#select count of no of emplyees in each dept and grpup by deptid\n",
    "\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "emp_count = emp_final.groupby(col(\"department_id\")).agg(count(\"employee_id\").alias(\"No of Employee\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9e303-5699-4125-bcc5-1c0ae1959446",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283af11-7373-4b85-ba54-dfebdcd2f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregation\n",
    "\n",
    "#select sum of salry as per dept emplyees in each dept and grpup by deptid\n",
    "\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "\n",
    "emp_sum = emp_final.groupby(\"department_id\").agg(sum(\"salary\").alias(\"Total salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19eeae-bb38-499b-a12c-3274f3ee5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f2f66-6ac0-4ae5-a936-9565437402df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregation\n",
    "\n",
    "#select avg of salry as per dept emplyees in each dept and grpup by deptid\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "emp_avg = emp_final.groupby(\"department_id\").agg(avg(\"salary\").alias(\"Avrage alary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dba35-3487-4064-876b-59d9c334f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c570d60-e749-4b69-9db2-8f7baa0cb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregation\n",
    "\n",
    "#select minimum and maximum of salry as per dept emplyees in each dept and grpup by deptid\n",
    "\n",
    "from pyspark.sql.functions import min,max\n",
    "\n",
    "emp_min = emp_final.groupby(\"department_id\").agg(min(\"salary\").alias(\"Minimum salary\"))\n",
    "\n",
    "emp_max = emp_final.groupby(\"department_id\").agg(max(\"salary\").alias(\"Maximum salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09374fc-7bcc-43df-ace2-09b7603d356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_min.show()\n",
    "emp_max.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48848d32-fe18-4a13-914a-8a83f7f1678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregation\n",
    "\n",
    "#select avg of salry as per dept emplyees in each dept and grpup by deptid and having salry > 50000\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "emp_avg_gt_50000 = emp_final.groupby(\"department_id\").agg(avg(\"salary\").alias(\"Avrage_salary\")).where(\"Avrage_salary > 50000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb86cb-6daf-41d1-bde6-25ecb6d7a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_avg_gt_50000.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3d002-3af1-4f91-b2d1-5a6f56f5353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data_2_other = emp2.select(\n",
    "    \"employee_id\", \n",
    "    \"salary\", \n",
    "    \"department_id\", \n",
    "    \"name\", \n",
    "    \"hire_date\", \n",
    "    \"gender\", \n",
    "    \"age\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71ea7d-1d4d-4637-aa75-978d2f1ce649",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp1.printSchema()\n",
    "emp_data_2_other.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5fb22f-b632-4be2-9a67-a349e8134eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_union_by_name  = emp1.unionByName(emp_data_2_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e26549-b734-4a7b-bf8f-aecf51e75284",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_union_by_name.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d741d-463c-4caa-a6c5-8293ea5d4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097d56b-a333-48f5-93bc-0690e4232a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
