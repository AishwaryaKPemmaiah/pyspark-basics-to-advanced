{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fedc05-0f77-4a5e-9adc-2cf1506defa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 11:21:39 WARN Utils: Your hostname, oem-Lenovo-G50-80 resolves to a loopback address: 127.0.1.1; using 172.18.0.1 instead (on interface br-dbb40503f1bc)\n",
      "25/05/20 11:21:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 11:21:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/20 11:21:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/20 11:21:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.\\\n",
    "         builder.\\\n",
    "         appName(\"Reading CSV\").\\\n",
    "         master(\"local[*]\").\\\n",
    "         getOrCreate ()\n",
    "        )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260a3383-af24-400f-88fc-f371c304e988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.1:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Reading CSV</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b3342206b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ec9d8f-31c1-4cf1-971c-4a61e880d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#read a CSV file\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"data/input/employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e954e95e-cbbc-4f2f-a466-219047f703c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "033e179e-56f2-41e5-9bdb-4eb9d20d51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a CSV file along with header\n",
    "\n",
    "df_header = spark.read.format(\"csv\").option(\"header\", True).load(\"data/input/employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c79fcd6-048b-4adf-a73b-8efd384d35fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_header.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23d864a-9881-45f0-8176-14164912f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a CSV file along with header\n",
    "\n",
    "df_header_with_inferSchema = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"data/input/employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b27a8d8-6867-4211-9d29-769d629564e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_header_with_inferSchema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3924371b-1541-4a8c-9f06-eb910e60c97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "|employee_id|department_id|       name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "|          1|          101| Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|          2|          102|  Bob Brown| 35|  Male| 55000|2014-01-10|\n",
      "|          3|          101|  Alice Lee| 30|Female| 48000|2018-05-21|\n",
      "|          4|          103| Jack Chang| 40|  Male| 60000|2013-04-01|\n",
      "|          5|          102|  Jill Wong| 28|Female| 50000|2017-01-05|\n",
      "|          6|          101| James Hong| 34|  Male| 52000|2016-07-15|\n",
      "|          7|          102|   Kate Kim| 29|Female| 51000|2019-08-01|\n",
      "|          8|          103|    Tom Tan| 36|  Male| 58000|2015-12-01|\n",
      "|          9|          101|   Lisa Lee| 27|Female| 47000|2017-11-01|\n",
      "|         10|          104| David Park| 38|  Male| 59000|2014-06-01|\n",
      "|         11|          105| Susan Chen| 33|Female| 56000|2016-03-01|\n",
      "|         12|          103|  Brian Kim| 41|  Male| 60000|2012-09-01|\n",
      "|         13|          107|  Emily Lee| 32|Female| 46000|2019-04-01|\n",
      "|         14|          106|Michael Lee| 29|  Male| 53000|2018-01-01|\n",
      "|         15|          108|Kelly Zhang| 31|Female| 54000|2017-06-01|\n",
      "|         16|          109|George Wang| 35|  Male| 57000|2015-03-01|\n",
      "|         17|          108|  Nancy Liu| 28|Female| 52000|2020-05-01|\n",
      "|         19|          107|Steven Chen| 30|  Male| 55000|2018-03-01|\n",
      "|         20|          102|  Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_header_with_inferSchema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ba627-630e-4e3f-b1e6-e55223020000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cde6f2d-a628-4819-9a8b-9f7579b1647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if schems is specified it doesnt need to go look the header or the records\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary string, hire_date string\"\n",
    "df_schema = spark.read.format(\"csv\").schema(emp_schema).\\\n",
    "            option(\"header\", False).load(\"data/input/employee_data.csv\")\n",
    "\n",
    "\n",
    "# important to specify c=schma of file resdinf sprk itslef will read wihch will cause probems when it reads data fom mutple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a3665c-37ac-4999-8374-e3371d4f0148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+----+------+------+----------+\n",
      "|employee_id|department_id|       name| age|gender|salary| hire_date|\n",
      "+-----------+-------------+-----------+----+------+------+----------+\n",
      "|       NULL|         NULL|       name|NULL|gender|salary| hire_date|\n",
      "|          1|          101| Jane Smith|  25|Female| 45000|2016-02-15|\n",
      "|          2|          102|  Bob Brown|  35|  Male| 55000|2014-01-10|\n",
      "|          3|          101|  Alice Lee|  30|Female| 48000|2018-05-21|\n",
      "|          4|          103| Jack Chang|  40|  Male| 60000|2013-04-01|\n",
      "|          5|          102|  Jill Wong|  28|Female| 50000|2017-01-05|\n",
      "|          6|          101| James Hong|  34|  Male| 52000|2016-07-15|\n",
      "|          7|          102|   Kate Kim|  29|Female| 51000|2019-08-01|\n",
      "|          8|          103|    Tom Tan|  36|  Male| 58000|2015-12-01|\n",
      "|          9|          101|   Lisa Lee|  27|Female| 47000|2017-11-01|\n",
      "|         10|          104| David Park|  38|  Male| 59000|2014-06-01|\n",
      "|         11|          105| Susan Chen|  33|Female| 56000|2016-03-01|\n",
      "|         12|          103|  Brian Kim|  41|  Male| 60000|2012-09-01|\n",
      "|         13|          107|  Emily Lee|  32|Female| 46000|2019-04-01|\n",
      "|         14|          106|Michael Lee|  29|  Male| 53000|2018-01-01|\n",
      "|         15|          108|Kelly Zhang|  31|Female| 54000|2017-06-01|\n",
      "|         16|          109|George Wang|  35|  Male| 57000|2015-03-01|\n",
      "|         17|          108|  Nancy Liu|  28|Female| 52000|2020-05-01|\n",
      "|         19|          107|Steven Chen|  30|  Male| 55000|2018-03-01|\n",
      "|         20|          102|  Grace Kim|  32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-----------+----+------+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0713df-35ff-4131-b16a-f289612bae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if schems is specified it doesnt need to go look the header or the records\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary string, hire_date string\"\n",
    "df_schema = spark.read.format(\"csv\").schema(emp_schema).\\\n",
    "            option(\"header\", True).load(\"data/input/employee_data.csv\")\n",
    "\n",
    "\n",
    "# important to specify c=schma of file resdinf sprk itslef will read wihch will cause probems when it reads data fom mutple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d8865e-1358-438f-8281-573174794534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "|employee_id|department_id|       name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "|          1|          101| Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|          2|          102|  Bob Brown| 35|  Male| 55000|2014-01-10|\n",
      "|          3|          101|  Alice Lee| 30|Female| 48000|2018-05-21|\n",
      "|          4|          103| Jack Chang| 40|  Male| 60000|2013-04-01|\n",
      "|          5|          102|  Jill Wong| 28|Female| 50000|2017-01-05|\n",
      "|          6|          101| James Hong| 34|  Male| 52000|2016-07-15|\n",
      "|          7|          102|   Kate Kim| 29|Female| 51000|2019-08-01|\n",
      "|          8|          103|    Tom Tan| 36|  Male| 58000|2015-12-01|\n",
      "|          9|          101|   Lisa Lee| 27|Female| 47000|2017-11-01|\n",
      "|         10|          104| David Park| 38|  Male| 59000|2014-06-01|\n",
      "|         11|          105| Susan Chen| 33|Female| 56000|2016-03-01|\n",
      "|         12|          103|  Brian Kim| 41|  Male| 60000|2012-09-01|\n",
      "|         13|          107|  Emily Lee| 32|Female| 46000|2019-04-01|\n",
      "|         14|          106|Michael Lee| 29|  Male| 53000|2018-01-01|\n",
      "|         15|          108|Kelly Zhang| 31|Female| 54000|2017-06-01|\n",
      "|         16|          109|George Wang| 35|  Male| 57000|2015-03-01|\n",
      "|         17|          108|  Nancy Liu| 28|Female| 52000|2020-05-01|\n",
      "|         19|          107|Steven Chen| 30|  Male| 55000|2018-03-01|\n",
      "|         20|          102|  Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af43b22f-7285-46d5-b842-0fe7451f6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Bad Records\n",
    "\n",
    "# Using Permsissive   _ Default Mode ---- > stores in  new column called corrupt_record\n",
    "\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary string, hire_date string\"\n",
    "df_permissive = spark.read.format(\"csv\").schema(emp_schema).option(\"header\", True).load(\"data/input/emp_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4eab133-4390-4e39-9509-635c942de349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-01-10|\n",
      "|          4|          101|    Alice Lee| 30|Female| 48000|2018-05-21|\n",
      "|          5|          103|    Jack Chan| 40|  Male|   Low|2013-04-01|\n",
      "|          6|          102|    Jill Wong| 28|Female| 50000|    nodate|\n",
      "|          7|          103|James Johnson| 34|  Male| 52000|2016-07-15|\n",
      "|          8|          102|     Kate Kim| 29|Female|   Low|    nodate|\n",
      "|          9|          103|      Tom Tan| 36|  Male| 58000|2015-12-01|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2017-11-01|\n",
      "|         11|          105|   David Park| 38|  Male| 59000|    nodate|\n",
      "|         12|          103|   Susan Chen| 33|Female| 56000|2016-03-01|\n",
      "|         13|          103|    Brian Doe| 41|  Male|   Low|2012-09-01|\n",
      "|         14|          107|    Emily Lee| 32|Female| 46000|2019-04-01|\n",
      "|         15|          106|  Michael Lee| 29|  Male| 53000|2018-01-01|\n",
      "|         16|          108|  Kelly Zhang| 31|Female| 54000|2017-06-01|\n",
      "|         17|          109|  George Wang| 35|  Male| 57000|2015-03-01|\n",
      "|         18|          108|    Nancy Liu| 28|Female| 52000|    nodate|\n",
      "|         19|          107|  Steven Chen| 30|  Male|   Low|2018-03-01|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_permissive.printSchema()\n",
    "df_permissive.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "534b2a66-8675-485b-bba4-6dded825474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Bad Records\n",
    "\n",
    "# Using Permsissive   _ Default Mode ---- > stores in  new column called corrupt_record\n",
    "\n",
    "#here shows null\n",
    "\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
    "df_permissive_null = spark.read.format(\"csv\").schema(emp_schema).option(\"header\", True).load(\"data/input/emp_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d77b2259-bb34-4ace-850a-b9835a1dcf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "|employee_id|department_id|         name|age|gender| salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n",
      "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n",
      "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-01-10|\n",
      "|          4|          101|    Alice Lee| 30|Female|48000.0|2018-05-21|\n",
      "|          5|          103|    Jack Chan| 40|  Male|   NULL|2013-04-01|\n",
      "|          6|          102|    Jill Wong| 28|Female|50000.0|      NULL|\n",
      "|          7|          103|James Johnson| 34|  Male|52000.0|2016-07-15|\n",
      "|          8|          102|     Kate Kim| 29|Female|   NULL|      NULL|\n",
      "|          9|          103|      Tom Tan| 36|  Male|58000.0|2015-12-01|\n",
      "|         10|          104|     Lisa Lee| 27|Female|47000.0|2017-11-01|\n",
      "|         11|          105|   David Park| 38|  Male|59000.0|      NULL|\n",
      "|         12|          103|   Susan Chen| 33|Female|56000.0|2016-03-01|\n",
      "|         13|          103|    Brian Doe| 41|  Male|   NULL|2012-09-01|\n",
      "|         14|          107|    Emily Lee| 32|Female|46000.0|2019-04-01|\n",
      "|         15|          106|  Michael Lee| 29|  Male|53000.0|2018-01-01|\n",
      "|         16|          108|  Kelly Zhang| 31|Female|54000.0|2017-06-01|\n",
      "|         17|          109|  George Wang| 35|  Male|57000.0|2015-03-01|\n",
      "|         18|          108|    Nancy Liu| 28|Female|52000.0|      NULL|\n",
      "|         19|          107|  Steven Chen| 30|  Male|   NULL|2018-03-01|\n",
      "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_permissive_null.printSchema()\n",
    "df_permissive_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "633e8120-bbff-441a-a5bc-bcffb1bc03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Bad Records\n",
    "\n",
    "# Using Permsissive   _ Default Mode ---- > stores in  new column called corrupt_record\n",
    "\n",
    "#here shows null\n",
    "\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date, _corrupt_record string\"\n",
    "df_permissive_null_cr = spark.read.format(\"csv\").schema(emp_schema).option(\"header\", True).load(\"data/input/emp_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b754dd0-06ab-4cc1-ba66-6a8537420414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "|employee_id|department_id|         name|age|gender| salary| hire_date|     _corrupt_record|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|                NULL|\n",
      "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|                NULL|\n",
      "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-01-10|                NULL|\n",
      "|          4|          101|    Alice Lee| 30|Female|48000.0|2018-05-21|                NULL|\n",
      "|          5|          103|    Jack Chan| 40|  Male|   NULL|2013-04-01|005,103,Jack Chan...|\n",
      "|          6|          102|    Jill Wong| 28|Female|50000.0|      NULL|006,102,Jill Wong...|\n",
      "|          7|          103|James Johnson| 34|  Male|52000.0|2016-07-15|                NULL|\n",
      "|          8|          102|     Kate Kim| 29|Female|   NULL|      NULL|008,102,Kate Kim,...|\n",
      "|          9|          103|      Tom Tan| 36|  Male|58000.0|2015-12-01|                NULL|\n",
      "|         10|          104|     Lisa Lee| 27|Female|47000.0|2017-11-01|                NULL|\n",
      "|         11|          105|   David Park| 38|  Male|59000.0|      NULL|011,105,David Par...|\n",
      "|         12|          103|   Susan Chen| 33|Female|56000.0|2016-03-01|                NULL|\n",
      "|         13|          103|    Brian Doe| 41|  Male|   NULL|2012-09-01|013,103,Brian Doe...|\n",
      "|         14|          107|    Emily Lee| 32|Female|46000.0|2019-04-01|                NULL|\n",
      "|         15|          106|  Michael Lee| 29|  Male|53000.0|2018-01-01|                NULL|\n",
      "|         16|          108|  Kelly Zhang| 31|Female|54000.0|2017-06-01|                NULL|\n",
      "|         17|          109|  George Wang| 35|  Male|57000.0|2015-03-01|                NULL|\n",
      "|         18|          108|    Nancy Liu| 28|Female|52000.0|      NULL|018,108,Nancy Liu...|\n",
      "|         19|          107|  Steven Chen| 30|  Male|   NULL|2018-03-01|019,107,Steven Ch...|\n",
      "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|                NULL|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_permissive_null_cr.printSchema()\n",
    "df_permissive_null_cr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d393cec1-a9a3-4766-a94f-84df245c55dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+-------+----------+--------------------+\n",
      "|employee_id|department_id|       name|age|gender| salary| hire_date|     _corrupt_record|\n",
      "+-----------+-------------+-----------+---+------+-------+----------+--------------------+\n",
      "|          5|          103|  Jack Chan| 40|  Male|   NULL|2013-04-01|005,103,Jack Chan...|\n",
      "|          6|          102|  Jill Wong| 28|Female|50000.0|      NULL|006,102,Jill Wong...|\n",
      "|          8|          102|   Kate Kim| 29|Female|   NULL|      NULL|008,102,Kate Kim,...|\n",
      "|         11|          105| David Park| 38|  Male|59000.0|      NULL|011,105,David Par...|\n",
      "|         13|          103|  Brian Doe| 41|  Male|   NULL|2012-09-01|013,103,Brian Doe...|\n",
      "|         18|          108|  Nancy Liu| 28|Female|52000.0|      NULL|018,108,Nancy Liu...|\n",
      "|         19|          107|Steven Chen| 30|  Male|   NULL|2018-03-01|019,107,Steven Ch...|\n",
      "+-----------+-------------+-----------+---+------+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how to identify corrupt records\n",
    "df_permissive_null_cr.where(\"_corrupt_record is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d089ff27-b9cc-4fa7-9c9c-73852166eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide own columns names\n",
    "\n",
    "\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date, bad_record string\"\n",
    "df_permissive_null_bad_rec= spark.read.format(\"csv\").schema(emp_schema).option(\"columnNameOfCorruptRecord\", \"bad_record\").option(\"header\", True).load(\"data/input/emp_data.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "989740af-e3ac-44d4-babd-c81c44505579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- bad_record: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "|employee_id|department_id|         name|age|gender| salary| hire_date|          bad_record|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|                NULL|\n",
      "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|                NULL|\n",
      "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-01-10|                NULL|\n",
      "|          4|          101|    Alice Lee| 30|Female|48000.0|2018-05-21|                NULL|\n",
      "|          5|          103|    Jack Chan| 40|  Male|   NULL|2013-04-01|005,103,Jack Chan...|\n",
      "|          6|          102|    Jill Wong| 28|Female|50000.0|      NULL|006,102,Jill Wong...|\n",
      "|          7|          103|James Johnson| 34|  Male|52000.0|2016-07-15|                NULL|\n",
      "|          8|          102|     Kate Kim| 29|Female|   NULL|      NULL|008,102,Kate Kim,...|\n",
      "|          9|          103|      Tom Tan| 36|  Male|58000.0|2015-12-01|                NULL|\n",
      "|         10|          104|     Lisa Lee| 27|Female|47000.0|2017-11-01|                NULL|\n",
      "|         11|          105|   David Park| 38|  Male|59000.0|      NULL|011,105,David Par...|\n",
      "|         12|          103|   Susan Chen| 33|Female|56000.0|2016-03-01|                NULL|\n",
      "|         13|          103|    Brian Doe| 41|  Male|   NULL|2012-09-01|013,103,Brian Doe...|\n",
      "|         14|          107|    Emily Lee| 32|Female|46000.0|2019-04-01|                NULL|\n",
      "|         15|          106|  Michael Lee| 29|  Male|53000.0|2018-01-01|                NULL|\n",
      "|         16|          108|  Kelly Zhang| 31|Female|54000.0|2017-06-01|                NULL|\n",
      "|         17|          109|  George Wang| 35|  Male|57000.0|2015-03-01|                NULL|\n",
      "|         18|          108|    Nancy Liu| 28|Female|52000.0|      NULL|018,108,Nancy Liu...|\n",
      "|         19|          107|  Steven Chen| 30|  Male|   NULL|2018-03-01|019,107,Steven Ch...|\n",
      "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|                NULL|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_permissive_null_bad_rec.printSchema()\n",
    "df_permissive_null_bad_rec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a52cb22b-62d3-4423-96ec-800f7435b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Bad Records\n",
    "\n",
    "# Using Dropmalformed   -> drops malformed records\n",
    "\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
    "df_malformed = spark.read.format(\"csv\").schema(emp_schema).option(\"mode\",\"dropMalformed\").option(\"header\", True).load(\"data/input/emp_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35c898ac-1292-4101-a87b-72f2c052d5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "|employee_id|department_id|         name|age|gender| salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n",
      "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n",
      "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-01-10|\n",
      "|          4|          101|    Alice Lee| 30|Female|48000.0|2018-05-21|\n",
      "|          7|          103|James Johnson| 34|  Male|52000.0|2016-07-15|\n",
      "|          9|          103|      Tom Tan| 36|  Male|58000.0|2015-12-01|\n",
      "|         10|          104|     Lisa Lee| 27|Female|47000.0|2017-11-01|\n",
      "|         12|          103|   Susan Chen| 33|Female|56000.0|2016-03-01|\n",
      "|         14|          107|    Emily Lee| 32|Female|46000.0|2019-04-01|\n",
      "|         15|          106|  Michael Lee| 29|  Male|53000.0|2018-01-01|\n",
      "|         16|          108|  Kelly Zhang| 31|Female|54000.0|2017-06-01|\n",
      "|         17|          109|  George Wang| 35|  Male|57000.0|2015-03-01|\n",
      "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_malformed.printSchema()\n",
    "df_malformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75922c46-c831-412d-bdb6-49d633dce804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb1fe59c-ced8-46a4-add6-83d5fe89d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Bad Records\n",
    "# Using FailFast   ---> Throws exception\n",
    "\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
    "df_failfast = spark.read.format(\"csv\").schema(emp_schema).option(\"mode\",\"failFast\").option(\"header\", True).load(\"data/input/emp_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56692e5e-519b-4c2b-946e-7d2feaaa5511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 15:12:50 ERROR Executor: Exception in task 0.0 in stage 20.0 (TID 20)\n",
      "org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [5,103,Jack Chan,40,Male,null,15796].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 26 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"Low\"\n",
      "\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n",
      "\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n",
      "\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n",
      "\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n",
      "\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n",
      "\t... 29 more\n",
      "25/05/20 15:12:50 WARN TaskSetManager: Lost task 0.0 in stage 20.0 (TID 20) (172.18.0.1 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [5,103,Jack Chan,40,Male,null,15796].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 26 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"Low\"\n",
      "\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n",
      "\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n",
      "\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n",
      "\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n",
      "\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n",
      "\t... 29 more\n",
      "\n",
      "25/05/20 15:12:50 ERROR TaskSetManager: Task 0 in stage 20.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o624.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 20) (172.18.0.1 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [5,103,Jack Chan,40,Male,null,15796].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [5,103,Jack Chan,40,Male,null,15796].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17178/2145739527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_failfast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_failfast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o624.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 20) (172.18.0.1 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [5,103,Jack Chan,40,Male,null,15796].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [5,103,Jack Chan,40,Male,null,15796].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "df_failfast.printSchema()\n",
    "df_failfast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e0444-565b-4ec2-a8c9-a48c5bfabe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c65c68-94e2-41d9-a0d4-d2e500a180d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd337329-8315-428d-92a9-a98d0bf902a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043fd5fc-0fd7-452b-93e9-7afa0b1359a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
